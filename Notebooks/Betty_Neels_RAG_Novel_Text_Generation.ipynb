{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PDF Preprocessing (Extract & Clean Text)**\n",
        "* Convert PDFs to text\n",
        "  - Use PyMuPDF or pdfplumber to extract text.\n",
        "* Clean & Structure the Text\n",
        "  - Remove headers, footers, and page numbers.\n",
        "  - Split text into chapters or meaningful chunks (~512-2048 tokens each).\n",
        "  - Convert text into structured format (JSON, Markdown, or plain text).\n",
        "* Tokenize for Embedding Model\n",
        "  - Use a tokenizer (like sentence-transformers or tiktoken) to ensure cleanhunking for embeddings.\n",
        "\n",
        "**Storing Data in a Vector Database (ChromaDB)**\n",
        "* Select an Embedding Model\n",
        "sentence-transformers (local, e.g., all-MiniLM-L6-v2)\n",
        "* Generate & Store Embeddings\n",
        "  - Convert text chunks into dense vectors.\n",
        "  - Store in ChromaDB.\n",
        "* Metadata Storage\n",
        "   - Keep chapter titles, book names, and page numbers for context retrieval.\n",
        "\n",
        "**Structured Prompting & Querying the Database**\n",
        "* User Input: The model receives a structured prompt.\n",
        "* Search Vector Database:\n",
        "  - Convert input into an embedding.\n",
        "   - Retrieve the top-k most relevant passages.\n",
        "* Construct a Contextual Prompt:\n",
        "  - Use retrieved passages to create a prompt with in-context learning (ICL).\n",
        "\n",
        "**Generative Model (LLaMA 2) for Novel Writing**\n",
        "* Model Selection:\n",
        "  - LLaMA 2 fine-tuned version for stylistic improvements).\n",
        "* Generation Process:\n",
        "  - Feed structured prompt into LLaMA 2.\n",
        "  - Use temperature & top-k sampling for creative output.\n",
        "* Iterative Refinement:\n",
        "  - Generate chapters iteratively rather than the full novel at once."
      ],
      "metadata": {
        "id": "XiqConCrjVKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IiXO_lnyXcME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Tokenization**"
      ],
      "metadata": {
        "id": "etVSLinZX5pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf pdfplumber tiktoken nltk"
      ],
      "metadata": {
        "id": "-U_vxx7CYCfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Define input/output directories\n",
        "input_folder = \"/content/drive/My Drive/Portfolio/Novel_RAG_Project/Data/Chunked_Texts\"\n",
        "output_folder = \"/content/drive/My Drive/Portfolio/Novel_RAG_Project/Data/tokenized_BettyNeelsDataset\"\n",
        "\n",
        "# Ensure output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Load embedding model (which includes a tokenizer)\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Process each chunked text file\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\"_chunked.txt\"):  # Ensure we're processing chunked files only\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, filename.replace(\"_chunked.txt\", \"_tokenized.txt\"))\n",
        "\n",
        "        with open(input_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Split chunks based on delimiter\n",
        "        chunks = text.split(\"\\n\\n### CHUNK \")\n",
        "\n",
        "        tokenized_chunks = []\n",
        "        for chunk_id, chunk in enumerate(chunks):\n",
        "            if chunk.strip():\n",
        "                tokens = model.tokenizer.tokenize(chunk)  # Tokenize text\n",
        "                tokenized_text = \" \".join(tokens)  # Convert list of tokens back to text\n",
        "                tokenized_chunks.append(f\"### CHUNK {chunk_id+1} ###\\n{tokenized_text}\")\n",
        "\n",
        "        # Join tokenized chunks with blank lines and save to new file\n",
        "        with open(output_path, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.write(\"\\n\\n\".join(tokenized_chunks))\n",
        "\n",
        "        print(f\"Tokenized and saved: {filename} → {output_path}\")\n",
        "\n",
        "print(\"All Files Tokenized.\")"
      ],
      "metadata": {
        "id": "IBcv7x6dXNgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Load LLaMA 2**"
      ],
      "metadata": {
        "id": "20dtrGauYKyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub chromadb sentence-transformers"
      ],
      "metadata": {
        "id": "Sx6rrAK4YYTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import chromadb\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "\n",
        "# Define input folder (Chunked text files)\n",
        "input_folder = \"/content/drive/My Drive/Portfolio/Novel_RAG_Project/Data/Chunked_Texts\"\n",
        "\n",
        "# Load embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize ChromaDB client & collection\n",
        "client = chromadb.PersistentClient(path=\"/content/chroma_db\")  # Persistent storage\n",
        "collection = client.get_or_create_collection(name=\"betty_neels_books\")\n",
        "\n",
        "# Load LLaMA model\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "\n",
        "# Download model\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# Initialize LLaMA model\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=4,  # Increase CPU cores if available\n",
        "    n_batch=512,  # Adjust for GPU VRAM\n",
        "    n_gpu_layers=32  # Optimize based on available GPU memory\n",
        ")\n",
        "\n",
        "print(\"LLaMA model loaded successfully.\")"
      ],
      "metadata": {
        "id": "S4OG8SblYJzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Extract and Store Embeddings, Model Prompting**"
      ],
      "metadata": {
        "id": "X1RxQf3jY1ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve relevant chunks from ChromaDB\n",
        "def retrieve_relevant_chunks(query, top_k=3):\n",
        "    query_embedding = embedding_model.encode(query)  # Convert query into vector\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k  # Retrieve top-k most relevant passages\n",
        "    )\n",
        "\n",
        "    retrieved_chunks = []\n",
        "    if \"documents\" in results and results[\"documents\"]:\n",
        "        for i in range(len(results[\"documents\"][0])):  # Loop through top-k results\n",
        "            chunk_text = results[\"documents\"][0][i]\n",
        "            metadata = results[\"metadatas\"][0][i]  # Metadata for context\n",
        "            retrieved_chunks.append((chunk_text, metadata))\n",
        "\n",
        "    return retrieved_chunks\n",
        "\n",
        "# Function to construct the structured prompt\n",
        "def construct_prompt(query, retrieved_chunks):\n",
        "    prompt = \"You are a writer skilled in the style of Betty Neels. Use the provided passages as inspiration to generate a new novel passage.\\n\\n\"\n",
        "    prompt += f\"### User Prompt: {query}\\n\\n\"\n",
        "\n",
        "    for i, (chunk, metadata) in enumerate(retrieved_chunks):\n",
        "        prompt += f\"### Retrieved Passage {i+1} from {metadata['book_name']}, {metadata['chapter_title']}:\\n\"\n",
        "        prompt += chunk + \"\\n\\n\"\n",
        "\n",
        "    prompt += \"### Now generate a continuation based on the style and content above.\"\n",
        "    return prompt\n",
        "\n",
        "# Function to generate text using LLaMA 2\n",
        "def generate_text(prompt, max_tokens=500):\n",
        "    output = lcpp_llm(\n",
        "        prompt,\n",
        "        max_tokens=max_tokens,  # Limit response length\n",
        "        temperature=0.7,  # Adjust creativity\n",
        "        top_p=0.9\n",
        "    )\n",
        "    return output[\"choices\"][0][\"text\"]\n",
        "\n",
        "# RAG query\n",
        "user_query = \"Write a chapter in Betty Neels’ style about a nurse meeting a wealthy doctor in Holland.\"\n",
        "\n",
        "retrieved_chunks = retrieve_relevant_chunks(user_query, top_k=3)  # Retrieve relevant passages\n",
        "structured_prompt = construct_prompt(user_query, retrieved_chunks)  # Build prompt\n",
        "generated_text = generate_text(structured_prompt)  # Generate output\n",
        "\n",
        "# Display the output\n",
        "print(\"\\n Generated Text:\\n\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "q7db3n2zY0XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT**\n",
        "\n",
        "Generated Text:\n",
        "\n",
        "Passage 1:\n",
        "\n",
        "\"The sun was setting over the windmills of Holland, casting a golden glow over the landscape. Nurse Emily walked down the cobblestone street, her starched cap and crisp white apron fluttering in the breeze. She had just finished her shift at the local hospital and was looking forward to a well-deserved rest. As she passed by the grand manor house on the hill, she noticed a handsome doctor standing in the doorway, watching her with piercing blue eyes.\"\n",
        "\n",
        "Passage 2:\n",
        "\n",
        "\"Dr. van der Meer was not only one of the most eligible bachelors in Holland, but also one of its most skilled physicians. He had built his reputation on his exceptional bedside manner and his ability to heal even the most stubborn cases. But as he gazed at Nurse Emily, he felt a stirring in his chest that he had never experienced before. She was not like the other nurses, with their curly hair and rosy cheeks. No, Emily was different - her dark hair was pulled back into a neat cap, revealing her high forehead and piercing brown eyes. He found himself drawn to her intelligence and self-assurance.\"\n",
        "\n",
        "Continuation:\n",
        "\n",
        "As Nurse Emily approached the manor house, she felt a sense of unease wash over her. She had heard whispers about Dr. van der Meer's reputation as a ladies' man, and she didn't want to be just another notch on his belt. But as she entered the grand foyer, she was struck by the warmth in his eyes. He greeted her with a bow, his deep voice sending shivers down her spine.\n",
        "\n",
        "\"Welcome, Nurse Emily,\" he said, offering his hand. \"I have been expecting you. I have a special task for you, one that requires not only your nursing skills but also your discretion"
      ],
      "metadata": {
        "id": "p9cSaIVfdZVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve relevant chunks from ChromaDB\n",
        "def retrieve_relevant_chunks(query, top_k=3):\n",
        "    query_embedding = embedding_model.encode(query)  # Convert query into vector\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k  # Retrieve top-k most relevant passages\n",
        "    )\n",
        "\n",
        "    retrieved_chunks = []\n",
        "    if \"documents\" in results and results[\"documents\"]:\n",
        "        for i in range(len(results[\"documents\"][0])):  # Loop through top-k results\n",
        "            chunk_text = results[\"documents\"][0][i]\n",
        "            metadata = results[\"metadatas\"][0][i]  # Metadata for context\n",
        "            retrieved_chunks.append((chunk_text, metadata))\n",
        "\n",
        "    return retrieved_chunks\n",
        "\n",
        "# 🔹 Function to construct the structured prompt\n",
        "def construct_prompt(query, retrieved_chunks):\n",
        "    prompt = \"You are a writer skilled in the style of Betty Neels. Use the provided passages as inspiration to generate a new novel passage.\\n\\n\"\n",
        "    prompt += f\"### User Prompt: {query}\\n\\n\"\n",
        "\n",
        "    for i, (chunk, metadata) in enumerate(retrieved_chunks):\n",
        "        prompt += f\"### Retrieved Passage {i+1} from {metadata['book_name']}, {metadata['chapter_title']}:\\n\"\n",
        "        prompt += chunk + \"\\n\\n\"\n",
        "\n",
        "    prompt += \"### Now generate a continuation based on the style and content above.\"\n",
        "    return prompt\n",
        "\n",
        "# 🔹 Function to generate text using LLaMA 2\n",
        "def generate_text(prompt, max_tokens=500):\n",
        "    output = lcpp_llm(\n",
        "        prompt,\n",
        "        max_tokens=max_tokens,  # Limit response length\n",
        "        temperature=0.7,  # Adjust creativity\n",
        "        top_p=0.9\n",
        "    )\n",
        "    return output[\"choices\"][0][\"text\"]\n",
        "\n",
        "# 🔹 Example RAG query\n",
        "user_query = \"Write a chapter in Betty Neels’ style about a nurse from London, going to Scotland to work with a doctor.\"\n",
        "\n",
        "retrieved_chunks = retrieve_relevant_chunks(user_query, top_k=3)  # Retrieve relevant passages\n",
        "structured_prompt = construct_prompt(user_query, retrieved_chunks)  # Build prompt\n",
        "generated_text = generate_text(structured_prompt)  # Generate output\n",
        "\n",
        "# 🔹 Display the output\n",
        "print(\"\\n📝 Generated Text:\\n\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "xdj6S9damU6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT:**\n",
        "\n",
        "Passage 1:\n",
        "\n",
        "\"The train chugged out of King's Cross station, carrying Emily, a young nurse from London, northwards to Scotland. She had accepted a position at a small hospital in the picturesque town of Drumnadrochit, nestled amongst the rugged hills and lochs of the Scottish Highlands. The journey was long, but Emily was filled with excitement at the prospect of starting her new life in this remote and beautiful place.\"\n",
        "\n",
        "Passage 2:\n",
        "\n",
        "\"As the train rumbled on, Emily couldn't help but feel a sense of nervous anticipation. She had never been to Scotland before, and the thought of working with Dr. MacTavish, the hospital's esteemed doctor, made her heart race. She had heard tales of his strict nature and high expectations, but she was determined to prove herself as a capable and dedicated nurse. The sound of the wind rustling through the trees outside her window only added to her sense of adventure.\"\n",
        "\n",
        "Passage 3:\n",
        "\n",
        "\"When the train finally arrived in Drumnadrochit, Emily was struck by the breathtaking beauty of the town. The hospital sat perched on a hill, its white walls and slate roof gleaming in the fading light of day. As she made her way to her new home, Emily couldn't help but feel a sense of wonder at the rugged landscape that surrounded her. She had never felt so alive, so ready for whatever challenges lay ahead.\"\n",
        "\n",
        "Continuation:\n",
        "\n",
        "As Emily settled into her small cottage on the hospital grounds, she couldn't help but feel a sense of trepidation about her new role. The other nurses seemed friendly enough, but she knew that Dr. MacTavish was notoriously difficult to work with. Still, she was determined to prove herself and make the most of this exciting new opportunity.\n",
        "\n",
        "The next morning, Emily reported for duty bright"
      ],
      "metadata": {
        "id": "trCaAIStmcrg"
      }
    }
  ]
}